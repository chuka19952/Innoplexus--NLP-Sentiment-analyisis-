# -*- coding: utf-8 -*-
"""innoplexus medical datahack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tmkHYH3DTqx_i-ufkL67R9A8Jczu0fKl
"""

# Natural Language Processing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from pandas import Series,DataFrame 
from datetime import date
import datetime
import io
from scipy import stats
import re
import nltk
from nltk.tokenize import word_tokenize as WordTokenizer
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

import re
from nltk.corpus import stopwords
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

train_set=pd.read_csv("/content/drive/My Drive/innoplexus/innoplex/train.csv")
test_set=pd.read_csv("/content/drive/My Drive/innoplexus/innoplex/test.csv")

#Creating and storing our target variable in a seperate dataframe
sentimentfrm=DataFrame(train_set["sentiment"])

#Here we want to merge our training and test set but for the columns to be same, we must first take of our target in train
train_set=train_set.drop(['sentiment'], axis=1)

#Now we merge BUT first we must create 2 train columns in our test and train set in order to id and seperate them later
train_set['train']=1
test_set['train']=0

combined=pd.concat([train_set, test_set])

"""One very important to note over here is that we haven’t specified the axis while concatenating which means we are combining along the rows.So what this will do is combine the test set below the train set with the ‘train’ column acting as the demarkation(all rows with 1 belong to train set and those with 0 to the test part).

Now do the encoding you require on the required column and save it in a new dataset."""

#Now we make our drug column into dummies as they turn out to have 2923 unique items/drugs
#NOW WE ENCODE
combined1=pd.get_dummies(combined['drug'], drop_first=True)

combined=combined.drop(['drug'], axis=1)

#combined=combined.drop(['unique_hash'], axis=1)
#combined=combined.drop(['train'], axis=1)

#removing any empty row or space
#combined['text'].dropna(inplace=True)

#preprocess , here we lowercase,split and stopword
def preprocess(raw_text):

    # keep only words
    letters_only_text = re.sub("[^a-zA-Z]", " ", raw_text)

    # convert to lower case and split 
    words = letters_only_text.lower().split()

    # remove stopwords
    stopword_set = set(stopwords.words("english"))
    meaningful_words = [w for w in words if w not in stopword_set]

    # join the cleaned words in a list
    cleaned_word_list = " ".join(meaningful_words)

    return cleaned_word_list

def process_data(dataset):
    #tweets_df = pd.read_csv(dataset,delimiter='|',header=None)

    num_tweets = combined.shape[0]
    print("Total tweets: " + str(num_tweets))

    cleaned_tweets = []
    print("Beginning processing of tweets at: ")

    for i in range(num_tweets):
        cleaned_tweet = preprocess(combined.iloc[i][1])
        cleaned_tweets.append(cleaned_tweet)
        if(i % 8200 == 0):
            print(str(i) + " tweets processed")

    print("Finished processing of tweets at: ")
    return cleaned_tweets

    print("Finished processing of tweets at: ")
    return cleaned_tweets

cleaned_data = process_data(combined["text"])

#cleaned_data=DataFrame(cleaned_data)

"""combined['text']=cleaned_data

#lemmitize the 'combined' dataframe
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
  return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]

combined['text'] = combined.text.apply(lemmatize_text)


#combined2= Series(combined.text)"""

"""#we want to create a function that plots descision regions
from matplotlib.colors import ListedColormap

def plot_decision_regions(X,y,classifier,test_idx=None,resolution=0.02):
    
    # Initialise the marker types and colors
    markers = ('s','x','o','^','v')
    colors = ('red','blue','lightgreen','gray','cyan')
    color_Map = ListedColormap(colors[:len(np.unique(y))]) #we take the color mapping correspoding to the 
                                                            #amount of classes in the target data
    
    # Parameters for the graph and decision surface
    x1_min = X[:,0].min() - 1
    x1_max = X[:,0].max() + 1
    x2_min = X[:,1].min() - 1
    x2_max = X[:,1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),
                           np.arange(x2_min,x2_max,resolution))
    
    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    
    plt.contour(xx1,xx2,Z,alpha=0.4,cmap = color_Map)
    plt.xlim(xx1.min(),xx1.max())
    plt.ylim(xx2.min(),xx2.max())
    
    # Plot samples
    X_test, Y_test = X[test_idx,:], y[test_idx]
    
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],
                    alpha = 0.8, c = color_Map(idx),
                    marker = markers[idx], label = cl
                   )
                        """

# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 42000)
combined3 = cv.fit_transform(cleaned_data).toarray()
combined3=DataFrame(combined3)

#RESET index before concating else it would bring error
combined1.reset_index(drop=True, inplace=True)
combined3.reset_index(drop=True, inplace=True)
combined4 = pd.concat([combined3, combined1], axis=1)

#join back the id and train columns to combine 4
combined4.reset_index(drop=True, inplace=True)
combined.reset_index(drop=True, inplace=True)
combined=combined.drop(['text'], axis=1)
combined=pd.concat([combined4, combined], axis=1)

#Now its time to separate those two datasets and we are done with both the train and test set now 
#containing the same number of columns.
train_df=combined[combined['train']==1]
test_df=combined[combined['train']==0]
#we now drop the train columns we created in test and train set
train_df.drop(['train'], axis=1, inplace=True)
test_df.drop(['train'], axis=1, inplace=True)

#now we combine back our target to our train set together
train_df=pd.concat([train_df,sentimentfrm], axis=1)

#dropping the unique_hash for both test_df and train_df
train_df.drop(['unique_hash'], axis=1, inplace=True)
test_df.drop(['unique_hash'], axis=1, inplace=True)

#seperate into variable
X=train_df.iloc[:, 0:42110].values
y=train_df.iloc[:, -1].values



# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import validation_curve

C_param_range = [0.001,0.01,0.1,1,10,100]

acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])
acc_table['C_parameter'] = C_param_range

plt.figure(figsize=(10, 10))

j = 0
for i in C_param_range:
    
    # Apply logistic regression model to training data
    lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)
    lr.fit(X_train,y_train)
    
    # Predict using model
    y_pred = lr.predict(X_test)
    
    # Saving accuracy score in table
    acc_table.iloc[j,1] = accuracy_score(y_test,y_pred)
    j += 1

acc_table



# Fitting Naive Bayes to the Training set
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

import sklearn
sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
sklearn.metrics.f1_score(y_test, y_pred, average ='macro')

#We now build our model  LOGISTIC REGRESSION
#checking accuracy
# Fitting Logistic Regression to the Training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)
# Predicting the Test set results
y_pred = classifier.predict(X_test)
#metrics
import sklearn
sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
sklearn.metrics.f1_score(y_test, y_pred, average ='macro')

# Fitting XGBoost to the Training set
from xgboost import XGBClassifier
classifier = XGBClassifier()
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)
#metrics
import sklearn
sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
sklearn.metrics.f1_score(y_test, y_pred, average ='macro')

# Fitting Decision Tree Classification to the Training set
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=10, random_state=0, criterion = 'entropy')
classifier.fit(X_train, y_train)
#predicting the test set
y_pred = classifier.predict(X_test)
#metrics
import sklearn
sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
sklearn.metrics.f1_score(y_test, y_pred, average ='macro')

# Fitting Random Forest Classification to the Training set
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)
# Predicting the Test set results
y_pred = classifier.predict(X_test)
#metrics
import sklearn
sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
sklearn.metrics.f1_score(y_test, y_pred, average ='macro')

# Applying k-Fold Cross Validation
from sklearn.model_selection import cross_val_score

from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
accuracies = cross_validate(estimator = classifier, X = X_train, y = y_train, cv = 10)
scoring = ['precision_score', 'recall_score', 'f1_score', 'accuracy_score']

# Predicting the Test set results and converting to csv
inno_test1 = classifier.predict(test_df)
#prediction = pd.DataFrame(inno_test1, columns=[0]).to_csv('innoptest8.csv')

