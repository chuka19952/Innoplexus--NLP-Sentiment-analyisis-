{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "innoplexus medical datahack.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYeJy_Rinxen",
        "colab_type": "code",
        "outputId": "3bb8e14c-7d08-4bdb-c647-81c1e56c0e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pandas import Series,DataFrame \n",
        "from datetime import date\n",
        "import datetime\n",
        "import io\n",
        "from scipy import stats\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize as WordTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDRIjdWVI8uU",
        "colab_type": "code",
        "outputId": "ab1efa45-c602-4b36-8906-835407886f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MxtLNtbJVAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set=pd.read_csv(\"/content/drive/My Drive/innoplexus/innoplex/train.csv\")\n",
        "test_set=pd.read_csv(\"/content/drive/My Drive/innoplexus/innoplex/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4DDlgxPKpDB",
        "colab_type": "code",
        "outputId": "ccc891e5-728c-458b-acf0-ea24a23ec695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Creating and storing our target variable in a seperate dataframe\n",
        "sentimentfrm=DataFrame(train_set[\"sentiment\"])\n",
        "\n",
        "#Here we want to merge our training and test set but for the columns to be same, we must first take of our target in train\n",
        "train_set=train_set.drop(['sentiment'], axis=1)\n",
        "\n",
        "#Now we merge BUT first we must create 2 train columns in our test and train set in order to id and seperate them later\n",
        "train_set['train']=1\n",
        "test_set['train']=0\n",
        "\n",
        "combined=pd.concat([train_set, test_set])\n",
        "\n",
        "\"\"\"One very important to note over here is that we haven’t specified the axis while concatenating which means we are combining along the rows.So what this will do is combine the test set below the train set with the ‘train’ column acting as the demarkation(all rows with 1 belong to train set and those with 0 to the test part).\n",
        "\n",
        "Now do the encoding you require on the required column and save it in a new dataset.\"\"\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One very important to note over here is that we haven’t specified the axis while concatenating which means we are combining along the rows.So what this will do is combine the test set below the train set with the ‘train’ column acting as the demarkation(all rows with 1 belong to train set and those with 0 to the test part).\\n\\nNow do the encoding you require on the required column and save it in a new dataset.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU8x55l6MWBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we make our drug column into dummies as they turn out to have 2923 unique items/drugs\n",
        "#NOW WE ENCODE\n",
        "combined1=pd.get_dummies(combined['drug'], drop_first=True)\n",
        "\n",
        "combined=combined.drop(['drug'], axis=1)\n",
        "\n",
        "#combined=combined.drop(['unique_hash'], axis=1)\n",
        "#combined=combined.drop(['train'], axis=1)\n",
        "\n",
        "#removing any empty row or space\n",
        "#combined['text'].dropna(inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2MrYc8lMdaH",
        "colab_type": "code",
        "outputId": "f04987ed-4ab3-40f8-9b4d-8781027d8e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#preprocess , here we lowercase,split and stopword\n",
        "def preprocess(raw_text):\n",
        "\n",
        "    # keep only words\n",
        "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
        "\n",
        "    # convert to lower case and split \n",
        "    words = letters_only_text.lower().split()\n",
        "\n",
        "    # remove stopwords\n",
        "    stopword_set = set(stopwords.words(\"english\"))\n",
        "    meaningful_words = [w for w in words if w not in stopword_set]\n",
        "\n",
        "    # join the cleaned words in a list\n",
        "    cleaned_word_list = \" \".join(meaningful_words)\n",
        "\n",
        "    return cleaned_word_list\n",
        "\n",
        "def process_data(dataset):\n",
        "    #tweets_df = pd.read_csv(dataset,delimiter='|',header=None)\n",
        "\n",
        "    num_tweets = combined.shape[0]\n",
        "    print(\"Total tweets: \" + str(num_tweets))\n",
        "\n",
        "    cleaned_tweets = []\n",
        "    print(\"Beginning processing of tweets at: \")\n",
        "\n",
        "    for i in range(num_tweets):\n",
        "        cleaned_tweet = preprocess(combined.iloc[i][1])\n",
        "        cleaned_tweets.append(cleaned_tweet)\n",
        "        if(i % 8200 == 0):\n",
        "            print(str(i) + \" tweets processed\")\n",
        "\n",
        "    print(\"Finished processing of tweets at: \")\n",
        "    return cleaned_tweets\n",
        "\n",
        "    print(\"Finished processing of tweets at: \")\n",
        "    return cleaned_tweets\n",
        "\n",
        "cleaned_data = process_data(combined[\"text\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total tweets: 8203\n",
            "Beginning processing of tweets at: \n",
            "0 tweets processed\n",
            "8200 tweets processed\n",
            "Finished processing of tweets at: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz_lHub7Mv3G",
        "colab_type": "code",
        "outputId": "13eb69a0-05cb-479a-e3b2-954b7d386249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#cleaned_data=DataFrame(cleaned_data)\n",
        "\n",
        "\"\"\"combined['text']=cleaned_data\n",
        "\n",
        "#lemmitize the 'combined' dataframe\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "combined['text'] = combined.text.apply(lemmatize_text)\n",
        "\n",
        "\n",
        "#combined2= Series(combined.text)\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"combined['text']=cleaned_data\\n\\n#lemmitize the 'combined' dataframe\\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\\nlemmatizer = nltk.stem.WordNetLemmatizer()\\n\\ndef lemmatize_text(text):\\n  return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\\n\\ncombined['text'] = combined.text.apply(lemmatize_text)\\n\\n\\n#combined2= Series(combined.text)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55N4k8QdBLQp",
        "colab_type": "code",
        "outputId": "7ef9bb1d-9868-4b6a-af0f-fa6a159f37e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "\"\"\"#we want to create a function that plots descision regions\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(X,y,classifier,test_idx=None,resolution=0.02):\n",
        "    \n",
        "    # Initialise the marker types and colors\n",
        "    markers = ('s','x','o','^','v')\n",
        "    colors = ('red','blue','lightgreen','gray','cyan')\n",
        "    color_Map = ListedColormap(colors[:len(np.unique(y))]) #we take the color mapping correspoding to the \n",
        "                                                            #amount of classes in the target data\n",
        "    \n",
        "    # Parameters for the graph and decision surface\n",
        "    x1_min = X[:,0].min() - 1\n",
        "    x1_max = X[:,0].max() + 1\n",
        "    x2_min = X[:,1].min() - 1\n",
        "    x2_max = X[:,1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),\n",
        "                           np.arange(x2_min,x2_max,resolution))\n",
        "    \n",
        "    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    \n",
        "    plt.contour(xx1,xx2,Z,alpha=0.4,cmap = color_Map)\n",
        "    plt.xlim(xx1.min(),xx1.max())\n",
        "    plt.ylim(xx2.min(),xx2.max())\n",
        "    \n",
        "    # Plot samples\n",
        "    X_test, Y_test = X[test_idx,:], y[test_idx]\n",
        "    \n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n",
        "                    alpha = 0.8, c = color_Map(idx),\n",
        "                    marker = markers[idx], label = cl\n",
        "                   )\n",
        "                        \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"#we want to create a function that plots descision regions\\nfrom matplotlib.colors import ListedColormap\\n\\ndef plot_decision_regions(X,y,classifier,test_idx=None,resolution=0.02):\\n    \\n    # Initialise the marker types and colors\\n    markers = ('s','x','o','^','v')\\n    colors = ('red','blue','lightgreen','gray','cyan')\\n    color_Map = ListedColormap(colors[:len(np.unique(y))]) #we take the color mapping correspoding to the \\n                                                            #amount of classes in the target data\\n    \\n    # Parameters for the graph and decision surface\\n    x1_min = X[:,0].min() - 1\\n    x1_max = X[:,0].max() + 1\\n    x2_min = X[:,1].min() - 1\\n    x2_max = X[:,1].max() + 1\\n    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),\\n                           np.arange(x2_min,x2_max,resolution))\\n    \\n    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\\n    Z = Z.reshape(xx1.shape)\\n    \\n    plt.contour(xx1,xx2,Z,alpha=0.4,cmap = color_Map)\\n    plt.xlim(xx1.min(),xx1.max())\\n    plt.ylim(xx2.min(),xx2.max())\\n    \\n    # Plot samples\\n    X_test, Y_test = X[test_idx,:], y[test_idx]\\n    \\n    for idx, cl in enumerate(np.unique(y)):\\n        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\\n                    alpha = 0.8, c = color_Map(idx),\\n                    marker = markers[idx], label = cl\\n                   )\\n                        \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT6w_4tdM3SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 42000)\n",
        "combined3 = cv.fit_transform(cleaned_data).toarray()\n",
        "combined3=DataFrame(combined3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGSsWfaLM-ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RESET index before concating else it would bring error\n",
        "combined1.reset_index(drop=True, inplace=True)\n",
        "combined3.reset_index(drop=True, inplace=True)\n",
        "combined4 = pd.concat([combined3, combined1], axis=1)\n",
        "\n",
        "#join back the id and train columns to combine 4\n",
        "combined4.reset_index(drop=True, inplace=True)\n",
        "combined.reset_index(drop=True, inplace=True)\n",
        "combined=combined.drop(['text'], axis=1)\n",
        "combined=pd.concat([combined4, combined], axis=1)\n",
        "\n",
        "#Now its time to separate those two datasets and we are done with both the train and test set now \n",
        "#containing the same number of columns.\n",
        "train_df=combined[combined['train']==1]\n",
        "test_df=combined[combined['train']==0]\n",
        "#we now drop the train columns we created in test and train set\n",
        "train_df.drop(['train'], axis=1, inplace=True)\n",
        "test_df.drop(['train'], axis=1, inplace=True)\n",
        "\n",
        "#now we combine back our target to our train set together\n",
        "train_df=pd.concat([train_df,sentimentfrm], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YePnuhZ8NFyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dropping the unique_hash for both test_df and train_df\n",
        "train_df.drop(['unique_hash'], axis=1, inplace=True)\n",
        "test_df.drop(['unique_hash'], axis=1, inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvAGT-QINOrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#seperate into variable\n",
        "X=train_df.iloc[:, 0:42110].values\n",
        "y=train_df.iloc[:, -1].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-LHZBuIQcuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xNC2TaoNWN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv_-a8iH8VCQ",
        "colab_type": "code",
        "outputId": "935401f7-3d69-406a-bf02-42a00390d2fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
        "\n",
        "acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\n",
        "acc_table['C_parameter'] = C_param_range\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "j = 0\n",
        "for i in C_param_range:\n",
        "    \n",
        "    # Apply logistic regression model to training data\n",
        "    lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n",
        "    lr.fit(X_train,y_train)\n",
        "    \n",
        "    # Predict using model\n",
        "    y_pred = lr.predict(X_test)\n",
        "    \n",
        "    # Saving accuracy score in table\n",
        "    acc_table.iloc[j,1] = accuracy_score(y_test,y_pred)\n",
        "    j += 1\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRjqUpXX-kP0",
        "colab_type": "code",
        "outputId": "a9b77088-e814-4330-a41c-6ac62b49874a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "acc_table"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>C_parameter</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.722727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.72803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.100</td>\n",
              "      <td>0.712121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.690152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.000</td>\n",
              "      <td>0.683333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100.000</td>\n",
              "      <td>0.686364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   C_parameter  Accuracy\n",
              "0        0.001  0.722727\n",
              "1        0.010   0.72803\n",
              "2        0.100  0.712121\n",
              "3        1.000  0.690152\n",
              "4       10.000  0.683333\n",
              "5      100.000  0.686364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDpLFm40_o8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJbr07tNNaty",
        "colab_type": "code",
        "outputId": "e1faf6b7-2546-4af1-c8c5-99ec431e04df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "import sklearn\n",
        "sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n",
        "sklearn.metrics.f1_score(y_test, y_pred, average ='macro')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.366910266027171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL8T5yt3NjdG",
        "colab_type": "code",
        "outputId": "d2d5c1e1-9801-47d3-8172-954edb9673fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#We now build our model  LOGISTIC REGRESSION\n",
        "#checking accuracy\n",
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(random_state = 0)\n",
        "classifier.fit(X_train, y_train)\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "#metrics\n",
        "import sklearn\n",
        "sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n",
        "sklearn.metrics.f1_score(y_test, y_pred, average ='macro')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45130489208707575"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7ySJ7dxKTod",
        "colab_type": "code",
        "outputId": "36838937-6434-4c28-da47-a26fde0b2d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "# Fitting XGBoost to the Training set\n",
        "from xgboost import XGBClassifier\n",
        "classifier = XGBClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "#metrics\n",
        "import sklearn\n",
        "sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n",
        "sklearn.metrics.f1_score(y_test, y_pred, average ='macro')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a9ca2f158ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Predicting the Test set results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4d6vwyMNsQZ",
        "colab_type": "code",
        "outputId": "5fd20187-1a35-4445-da72-ffe3c26cca25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fitting Decision Tree Classification to the Training set\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=10, random_state=0, criterion = 'entropy')\n",
        "classifier.fit(X_train, y_train)\n",
        "#predicting the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "#metrics\n",
        "import sklearn\n",
        "sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n",
        "sklearn.metrics.f1_score(y_test, y_pred, average ='macro')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40989704306055813"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pd2xWlMN25-",
        "colab_type": "code",
        "outputId": "40978b98-091d-4765-cc1e-640e063f2f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fitting Random Forest Classification to the Training set\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "#metrics\n",
        "import sklearn\n",
        "sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n",
        "sklearn.metrics.f1_score(y_test, y_pred, average ='macro')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3388160918402436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNdpcZjlOBba",
        "colab_type": "code",
        "outputId": "4d28ac2e-4d42-4203-fe68-6d51802a33ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Applying k-Fold Cross Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "accuracies = cross_validate(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
        "scoring = ['precision_score', 'recall_score', 'f1_score', 'accuracy_score']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
            "  % (min_groups, self.n_splits)), Warning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHwmO6BxOMX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results and converting to csv\n",
        "inno_test1 = classifier.predict(test_df)\n",
        "#prediction = pd.DataFrame(inno_test1, columns=[0]).to_csv('innoptest8.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgXLkwpkOwaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}